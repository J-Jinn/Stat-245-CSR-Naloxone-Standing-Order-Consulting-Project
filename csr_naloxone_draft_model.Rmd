---
title: "CSR Naloxone Standing Order Project - Draft Model(s)"
author: 'STAT 245, Fall 2020 (Group Members: Nana Ama Baidoo, Alex Visser, Joshua Ridder, Joseph Jinn)'
date: '`r format(Sys.Date(), "%B %d, %Y")`'
output: 
  pdf_document:
    fig_height: 8
    fig_width: 13
  html_document:
    fig_height: 8
    fig_width: 13
classoption: landscape
---

```{r, setup, include = FALSE}
# load packages that are going to be used
require(tidyverse)
require(ggformula)
require(mosaic)
require(fastR2)
require(s245)
require(pander)
require(DHARMa)
require(glmmTMB)
require(MuMIn)
require(car)
require(dplyr) # SQL syntax ftw.
require(readr)
# require(reticulate) # Utilize Python programming language.
require(ggeffects)

# Some customization.  You can alter or delete as desired (if you know what you are doing).

theme_set(theme_minimal(base_size=12))     # change theme for ggplot2/ggformula

knitr::opts_chunk$set(
  echo = TRUE,      # for homework, always show R code (this is the default)
  error = TRUE, # do not interrupt generation in case of errors,
  tidy = FALSE,     # display code as typed (rather than reformatted)
  size = "small",   # slightly smaller font for code
  message = FALSE, warning = FALSE) # don't print warnings or messages in compiled document. So you MUST check them in RStudio!
```

Note to self: Run "remotes::install_github('ProjectMOSAIC/ggformula')" to install development version of ggformula.

```{r}
# sessionInfo()
options(max.print = 6000) # Ensure we can print entire summary.
# remotes::install_github('ProjectMOSAIC/ggformula')
```

**This is a draft model to try to answer the research question: "Trends in overdose deaths in Kent County compared to other counties."**

**Update: Also fitting an additional model to answer another research question: "Compare cause of death including narcotics to non-narcotics."**

___

# Project Information

Research Question(s):

* Trends in overdose deaths in Kent County compared to other counties.
+ Do numbers seem to be increasing, decreasing, or staying the same after 2017?
+ Consider creating a map and tracking it over time

* Does gender, age, or race seem to have an effect on overdose rate?
+ Consider using metropolitan area as another variable.

* Compare cause of death including narcotics to non-narcotics. 
+ Has one increased as the other decreased over time?
+ Have both increased/decreased?

___

Trends in overdose deaths in Kent County compared to other counties.
>Response Variable: "deaths" (common to all queries)
Type of Response: Count data (negative binomial regression modeling or similar)

Does gender, age, or race seem to have an effect on overdose rate?
>Response Variable: "crude_rate" (deaths / population * 1000)
Type of Response: Continuous data (consider multiple linear regression)

Compare cause of death including narcotics to non-narcotics.
> Response variable: DeathType (create new factor variable) = (0 - Non-Drug, 1 - Drug)
Type of Response: We could convert this into a logistic (binary) regression problem by defining success = Drug Induced Causes and failure = Alcohol Induced Causes + All other non-drug and non-alcohol causes.

___

About how many predictors can you include, given the size of the data set?

>We have nearly 20K rows in our dataset.  But, only a small fraction are drug-related deaths.  This will depend on which research question we choose to pursue.

___

All potential predictors (not just the ones currently of interest).

Trends in overdose deaths in Kent County compared to other counties.
> "Gender", "Hispanic Origin", "Race", "Year", "Month", "Weekday", "Autopsy", "Place of Death", "Drug/Alcohol Induced Cause", "Single-Year Ages", "Five-Year Age Groups", "Ten-Year Age Groups", and "2013 Urbanization".

Does gender, age, or race seem to have an effect on overdose rate?
> "Gender", "Single-Year Ages","Five-Year Age Groups", "Ten-Year Age Groups", and "Race".
We can also consider "County, "Hispanic Origin", "Year", "Month", "Weekday", "Autopsy", "Place of Death", "Drug/Alcohol Induced Cause", , and "2013 Urbanization".

Compare cause of death including narcotics to non-narcotics.
> "County", "Gender", "Hispanic Origin", "Race", "Year", "Month", "Weekday", "Autopsy", "Place of Death", "Single-Year Ages", "Five-Year Age Groups", "Ten-Year Age Groups", and "2013 Urbanization".

___

All variables currently under consideration:

"County"
"Gender"
"Race"
"Year"
"Drug/Alcohol Induced Cause"
"Single-Year Ages",
"Five-Year Age Groups"
"Ten-Year Age Groups" (use broadest)


Common to all queries:

"deaths"
"population"
"crude_rate"
"percent_of_total_deaths"


Tentative:

"Hispanic Origin"
"Autopsy"
"Place of Death"

Ignore:

"2013 Urbanization" - classifies population density and other factors at the county level - pick between the 2006 or the 2013 NCHS Urban-Rural Classification Scheme for Counties. 

"Month"
"Weekday"


Considerations:

Differences in trends over time in .... (multiple panels or colors)
Number of overdose deaths affected by geographic and demographic groups.

___

>Trends in overdose deaths in Kent County compared to other counties.

Random Effects (deprecated based on instructor feedback):

* (1 | 2013 Urbanization) - So, there could be differences in the # of deaths between urbanization classes and we would also want to generalize to the population as a whole without necessarily referencing any specific type of location class.

* (1 | County) - Similar to 2013 Urbanization above.

* (1 | 2013 Urbanization / binned Age) - The # of deaths could vary between urbanization classes and between age groups within each urbanization class.

* (1 | County / binned Age) - The # of deaths could vary between county and between age groups within each county.


Interactions (could be additional ones):

* 2013 Urbanization and Place of Death - The type of location could determine the likely place of death as hospitals are more likely to be located in urban centers rather than rural areas, etc.

* 2013 Urbanization and Race - There could be demographic differences based on race as the socio-economic status of individuals could determine where they are more likely to live.

* 2013 Urbanization Population - The type of location probably has an effect on the population levels.  Urban areas are probably more densely populating than non-urban areas.

* Year and Population - The population of any given area definitely would change when accounting for the year.

* Place of Death and Autopsy - The place where the individual died could determine whether or not they receive an autopsy.  Then again, if there is a cause of death that probably means they did perform an autopsy already.

* Place of Death and Race - Depending on socio-economic status, some individuals might not be able to afford a hospice or nursing home.  Other similar reasoning.

* County and Place of Death - Similar to 2013 Urbanization and Place of Death.

* County and Race - Similar to 2013 Urbanization and Race.

* County and Population - Similar to 2013 Urbanization and Population.


Updated list of interactions based upon instructor feedback:

* Gender and Race?
* Year and County?


Offsets(when applicable): 

* Can offset "deaths" with "population" althought "crude _rate" = deaths / population * 1000.

___

>Does gender, age, or race seem to have an effect on overdose rate (a.k.a. "crude rate")?

Random Effects:

* None.


Interactions:

* Would be similar to above.


Offsets(when applicable):

* Not applicable.

___

>Compare cause of death including narcotics to non-narcotics.

Random Effects:

* None


Interactions:

* Would be similar to above.


Offsets(when applicable): 

* Not applicable.

___

# Data Tidying (yet again)

[readr documentation](https://www.rdocumentation.org/packages/readr/versions/1.3.1)

[read_table documentation](https://readr.tidyverse.org/reference/read_table.html)

[read_delim documentation](https://www.rdocumentation.org/packages/readr/versions/1.3.1/topics/read_delim)

[Data Tidying Tutorial](https://rsconnect.calvin.edu:3939/content/24/#section-other-data-file-formats)

```{r}
parse_query <- function(file_name_as_string) {
  data <- read_delim(file = file_name_as_string,
                     delim = "\t",
                     col_names = TRUE,
                     col_types = NULL,
                     na = "NA",
                     skip = 0,
                     n_max = Inf,
                     guess_max = 100,
                     progress = show_progress(),
                     comment =c("-"),
                     skip_empty_rows = TRUE
  )[, -c(1)]  %>% # Ignore the "Notes" column
    filter_all(any_vars(!is.na(.)))  # Remove all rows with ALL NAs in all columns (due to "Notes" section)
  return(data)
}
```

Parse the file(s).

```{r}
data_drug <- parse_query("datasets//joseph_jinn_queries//CDCWonderDataQuery_UnderlyingCauseOfDeath_GROUPBY_GenderRaceYearTenYearAgeGroupCounty_LIMITBY_noneDrugInducedCausesAllDates9decimalsPer1k.txt")
```

```{r}
glimpse(data_drug)
```

```{r}
data_alcohol <- parse_query("datasets//joseph_jinn_queries//CDCWonderDataQuery_UnderlyingCauseOfDeath_GROUPBY_GenderRaceYearTenYearAgeGroupCounty_LIMITBY_noneAlcoholInducedCausesAllDates9decimalsPer1k.txt")
```

```{r}
glimpse(data_alcohol)
```

Ignoring the errors. Error messages seem to indicate failing to parse the "Notes" section which we do not care about as they aren't data.

```{r}
data_other <- parse_query("datasets//joseph_jinn_queries//CDCWonderDataQuery_UnderlyingCauseOfDeath_GROUPBY_GenderRaceYearTenYearAgeGroupCounty_LIMITBY_noneAllOtherNonDrugNonAlcoholCausesAllDates9decimalsPer1k.txt")
```

```{r}
glimpse(data_other)
```

___

Add new variable specifying whether it was "alcohol", "drug", or "other" type of cause for death.

```{r}
data_drug <- data_drug %>% mutate(`Cause Of Death` = "drug")
data_alcohol <- data_alcohol %>% mutate(`Cause Of Death` = "alcohol")
data_other <- data_other %>% mutate(`Cause Of Death` = "other")
```


```{r}
glimpse(data_drug)
glimpse(data_alcohol)
glimpse(data_other)
```

___

Convert logical gender code to character gender code (R arbitrarily set "Female" = FALSE and "Male" = NA.

```{r}
head(data_drug, 5)
```

```{r}
tail(data_drug, 5)
```

___

```{r}
data_drug <- data_drug %>% mutate(`Gender Code` = case_when(`Gender` ==  'Female' ~ 'F',
                                                            TRUE ~ 'M'))
```

```{r}
head(data_drug, 5)
```

```{r}
tail(data_drug, 5)
```

```{r}
data_other <- data_other %>% mutate(`Gender Code` = case_when(`Gender` ==  'Female' ~ 'F',
                                                              TRUE ~ 'M'))
```

```{r}
head(data_other, 5)
```

```{r}
tail(data_other, 5)
```

___

[DPLYR joins](https://dplyr.tidyverse.org/reference/join.html)

[Join explanations using DPLYR](https://statisticsglobe.com/r-dplyr-join-inner-left-right-full-semi-anti)

Join the drug, alcohol, and other data for a complete dataset.

```{r}
data <- bind_rows(data_drug, data_alcohol, data_other)
```

```{r}
glimpse(data)
```

Ok, looking good so far.

___

[CDC Wonder Data Queries](https://wonder.cdc.gov/ucd-icd10.html)

[Other Data](https://www.cdc.gov/nchs/nvss/vsrr/drug-overdose-data.htm)

[Other Data](https://www.cdc.gov/drugoverdose/data/statedeaths/drug-overdose-death-2018.html)

[Basic Query Template](https://wonder.cdc.gov/controller/saved/D76/D95F739)

* Common shared columns (if included in result set):
+ Deaths
+ Population
+ `Crude Rate`
+ `Crude Rate Lower 95% Confidence Interval`
+ `Crude Rate Upper 95% Confidence Interval`
+ `Crude Rate Standard Error`
+ `% of Total Deaths`

Unique columns are based on what we group by (as seen in the dataframe above).

We should also rename columns to not have spaces and other special characters for ease with working with ggformula and other packages (so names aren't enclosed in "`").

Define function to replace "messy" column names with "clean" version.  Adjust in the future as necessary.

[R - Regular Expressions](https://rstudio.com/wp-content/uploads/2016/09/RegExCheatsheet.pdf)

```{r}
remove_whitespace_column_names <- function(my_dataframe) {
  
  return(names(my_dataframe) %>% str_replace_all("\\s", "_") %>% 
           str_replace_all("%", "percent") %>% 
           str_replace_all("/", "_") %>% 
           str_replace_all("-", "_") %>% 
           str_replace_all("^([0-9]+)", "c\\1") %>% # Replace beginning number with dummy c - character.
           tolower)
}
```

Clean/tidy the column names:

```{r}
colnames(data) <- remove_whitespace_column_names(data)
```

Check that "%", "-", and "/" have been removed, in addition to whitespace.

```{r}
glimpse(data)
```

___

If necessary:

Replace all rows with "Suppressed" in the "Deaths" column with randomly generated values between 0-9 since that is the criteria for when data values are labeled as "Suppressed".  

Replace all rows with "Suppressed" in the "Population" column with randomly generated values between 0-9 since that is the criteria for when data values are labeled as "Suppressed".

Crude rates are marked as "Unreliable" when death counts are less than 20.  We could manually create a crude rate by dividing "population" by "deaths" multiplied by X individuals.

Crude Rate = Count / Population * (per specified # of people)

Crude rates are marked as "Not Applicable" when "Population" is unavailable.

```{r}
count(data$deaths == "Suppressed")
count(data$population == "Suppressed")
count(data$crude_rate == "Suppressed")
count(data$crude_rate == "Unreliable")
count(data$crude_rate == "Not Applicable")
```

Good, we have no "Suppressed" rows but we do have a decent amount of "Unreliable" rows for crude rates.

Since the crude rates for our dataset were calculated per 1,000 people, we will use that for manual creation of crude rates for "Unreliable" rows.

**TODO: Compute crude rates for "Unreliable" rows and insert into dataset**

```{r}
data_temp <- data %>% mutate(crude_rate_added = case_when(
  crude_rate == 'Unreliable' ~ deaths / population * 1000,
  TRUE ~ as.numeric(crude_rate)
))
```

```{r}
glimpse(data_temp)
```

And double-check that our manual computations for crude_rate makes sense.

```{r}
# which(data$crude_rate == "Unreliable")
```

Row 9 and 12, among many others had "Unreliable" as the crude_rate.  With our manual computations of crude_rate, we seem to have obtained reasonable values.

Also, we have copied over crude_rate values for rows that had existing values for crude_rate successfully.  So, we are all set.

```{r}
head(data, 20)
```
```{r}
head(data_temp, 20)
```

And re-asssign temporary dataframe back to our original dataframe.

```{r}
data <- data_temp
```
___

Now, we should convert the data types for each attribute to the ones desired for model fitting.

```{r}
data <- data %>% mutate_if(is.character, as.factor)
```

```{r}
glimpse(data)
```

We will keep crude rate as a "factor" for now.  It does contain "Unreliable" for quite a few rows.

```{r}

```

And county_code, year, and year_code, should be a factor, not a numeric (double) type.

```{r}
data <- data %>% mutate(county_code = as.factor(county_code))
data <- data %>% mutate(year = as.factor(year))
data <- data %>% mutate(year_code = as.factor(year_code))
```

```{r}
glimpse(data)
```

And we want percent_of_total_deaths as a numeric type too not a factor or character with a % sign.

```{r}
data <- data %>% mutate(percent_of_total_deaths = as.numeric(gsub("%", "", percent_of_total_deaths)))
```

```{r}
glimpse(data)
```

And last but not least, "deaths" and "population" should be integer instead of double numeric sub-types.

```{r}
data <- data %>% mutate(deaths = as.integer(deaths))
data <- data %>% mutate(population = as.integer(population))
```

```{r}
glimpse(data)
```

Ok, things are looking good now.

But, we should also export out our cleaned and combined dataset to a CSV file for future use.

```{r}
write.csv2(data,"datasets\\joseph_jinn_queries\\CDCWonderDataQuery_UnderlyingCauseOfDeath_GROUPBY_GenderRaceYearTenYearAgeGroupCounty_LIMITBY_noneAllCausesOfDeathDates9decimalsPer1k_TIDY.csv", append = FALSE)
```

And test we can read everything back in correctly.

```{r}
import_test <- read_csv2("datasets\\joseph_jinn_queries\\CDCWonderDataQuery_UnderlyingCauseOfDeath_GROUPBY_GenderRaceYearTenYearAgeGroupCounty_LIMITBY_noneAllCausesOfDeathDates9decimalsPer1k_TIDY.csv")
```

```{r}
glimpse(import_test)
```

```{r}
head(import_test)
```

And everything looks good.

___

# EDA - Exploratory Data Analysis (will correct labels, etc., in the future)

These are initial EDA graphs for the entire dataset (drug, alcohol, and all non-drug/alchol causes of death).

```{r}
gf_bar(~gender, data = data, alpha = 0.25, color = "#087EC6",
       xlab="Gender",
       ylab="Number of Observations",
       title="CDC Wonder Underlying Cause of Death Dataset Queries",
       subtitle = "",
       caption = ""
) + labs(fill = "Vehicle Searched?") + gf_jitter()
```

```{r}
gf_bar(~race, data = data, alpha = 0.25, color = "#087EC6",
       xlab="Raceh",
       ylab="Number of Observations",
       title="CDC Wonder Underlying Cause of Death Dataset Queries",
       subtitle = "",
       caption = ""
) + coord_flip()
```

```{r}
gf_bar(~year, data = data, alpha = 0.25, color = "#087EC6",
       xlab="Year",
       ylab="Number of Observations",
       title="CDC Wonder Underlying Cause of Death Dataset Queries",
       subtitle = "",
       caption = ""
)
```

```{r}
gf_bar(~ten_year_age_groups, data = data, alpha = 0.25, color = "#087EC6",
       xlab="10 Year Age Groups",
       ylab="Number of Observations",
       title="CDC Wonder Underlying Cause of Death Dataset Queries",
       subtitle = "",
       caption = ""
) + coord_flip()
```

```{r}
gf_bar(~county, data = data, alpha = 0.25, color = "#087EC6",
       xlab="County",
       ylab="Number of Observations",
       title="CDC Wonder Underlying Cause of Death Dataset Queries",
       subtitle = "",
       caption = ""
) + coord_flip()
```

```{r}
gf_bar(~cause_of_death, data = data, alpha = 0.25, color = "#087EC6",
       xlab="Cause of Death",
       ylab="Number of Observations",
       title="CDC Wonder Underlying Cause of Death Dataset Queries",
       subtitle = "",
       caption = ""
)
```

```{r}
gf_point(deaths ~ population, data = data, alpha = 0.1, color = "#087EC6",
         xlab="Deaths",
         ylab="Population",
         title="CDC Wonder Underlying Cause of Death Dataset Queries",
         subtitle = "",
         caption = ""
)
```

```{r}
gf_point(deaths ~ crude_rate_added, data = data, alpha = 0.1, color = "#087EC6",
         xlab="Crude Rate Added",
         ylab="Deaths",
         title="CDC Wonder Underlying Cause of Death Dataset Queries",
         subtitle = "",
         caption = ""
)
```

```{r}
gf_point(deaths ~ percent_of_total_deaths, data = data, alpha = 0.25, color = "#087EC6",
         xlab="Percent of Total Deaths",
         ylab="Deaths",
         title="CDC Wonder Underlying Cause of Death Dataset Queries",
         subtitle = "",
         caption = ""
)
```

Quick and dirty EDA.

___


These are initial EDA graphs for the subset of the dataset with ONLY drug-related deaths.

```{r}
drug_deaths_only_data <- data %>% filter(data$cause_of_death == "drug")
```

```{r}
glimpse(drug_deaths_only_data)
```

```{r}
gf_bar(~gender, data = drug_deaths_only_data, alpha = 0.25, color = "#087EC6",
       xlab="Gender",
       ylab="Number of Observations",
       title="CDC Wonder Underlying Cause of Death Dataset Queries",
       subtitle = "",
       caption = ""
) + labs(fill = "Vehicle Searched?") + gf_jitter()
```

```{r}
gf_bar(~race, data = drug_deaths_only_data, alpha = 0.25, color = "#087EC6",
       xlab="Raceh",
       ylab="Number of Observations",
       title="CDC Wonder Underlying Cause of Death Dataset Queries",
       subtitle = "",
       caption = ""
) + coord_flip()
```

```{r}
gf_bar(~year, data = drug_deaths_only_data, alpha = 0.25, color = "#087EC6",
       xlab="Year",
       ylab="Number of Observations",
       title="CDC Wonder Underlying Cause of Death Dataset Queries",
       subtitle = "",
       caption = ""
)
```

```{r}
gf_bar(~ten_year_age_groups, data = drug_deaths_only_data, alpha = 0.25, color = "#087EC6",
       xlab="10 Year Age Groups",
       ylab="Number of Observations",
       title="CDC Wonder Underlying Cause of Death Dataset Queries",
       subtitle = "",
       caption = ""
) + coord_flip()
```

```{r}
gf_bar(~county, data = drug_deaths_only_data, alpha = 0.25, color = "#087EC6",
       xlab="County",
       ylab="Number of Observations",
       title="CDC Wonder Underlying Cause of Death Dataset Queries",
       subtitle = "",
       caption = ""
) + coord_flip()
```

```{r}
gf_bar(~cause_of_death, data = drug_deaths_only_data, alpha = 0.25, color = "#087EC6",
       xlab="Cause of Death",
       ylab="Number of Observations",
       title="CDC Wonder Underlying Cause of Death Dataset Queries",
       subtitle = "",
       caption = ""
)
```

```{r}
gf_point(deaths ~ population, data = drug_deaths_only_data, alpha = 0.1, color = "#087EC6",
         xlab="Deaths",
         ylab="Population",
         title="CDC Wonder Underlying Cause of Death Dataset Queries",
         subtitle = "",
         caption = ""
)
```

```{r}
gf_point(deaths ~ crude_rate_added, data = drug_deaths_only_data, alpha = 0.1, color = "#087EC6",
         xlab="Crude Rate Added",
         ylab="Deaths",
         title="CDC Wonder Underlying Cause of Death Dataset Queries",
         subtitle = "",
         caption = ""
)
```

```{r}
gf_point(deaths ~ percent_of_total_deaths, data = drug_deaths_only_data, alpha = 0.25, color = "#087EC6",
         xlab="Percent of Total Deaths",
         ylab="Deaths",
         title="CDC Wonder Underlying Cause of Death Dataset Queries",
         subtitle = "",
         caption = ""
)
```

___

# Model Fitting (first research question).

Redundant, but filtering again by only drug-related deaths for the first research question.

```{r}
drug_data <- data %>% filter(data$cause_of_death == "drug")
```

```{r}
glimpse(drug_data)
```

Compute the rule of thumb for maximum number of parameters:

```{r}
nrow(drug_data) / 15
```

So, we can have about 45 parameters at most in our model.

```{r}
length(levels(drug_data$gender)) - 1 + length(levels(drug_data$race)) - 1 + 
  length(levels(drug_data$year)) - 1 + length(levels(drug_data$ten_year_age_groups)) - 1 + 
  length(levels(drug_data$county)) - 1 + 1 + 1 + 1 # 1 each for population, percent_of_total_deaths, residuals
```
Too many, guess we'll have to remove "county", sadly.

```{r}
length(levels(drug_data$gender)) - 1 + length(levels(drug_data$race)) - 1 + 
  length(levels(drug_data$year)) - 1 + length(levels(drug_data$ten_year_age_groups)) - 1 + 
  1 + 1 + 1 # 1 each for population, percent_of_total_deaths, residuals
```

And now we're good.

We can now use a negative binomial regression model for our count data, "deaths".

```{r}
model <- glmmTMB(deaths ~ gender + race + year + ten_year_age_groups +
                   population + percent_of_total_deaths,
                 data = drug_data, 
                 family = nbinom2(link = "log"))
```

```{r}
summary(model)
```

And we should probably scale our predictors try to resolve some NA issues with coefficients.

```{r}
min(drug_data$deaths)
max(drug_data$deaths)
min(drug_data$population)
max(drug_data$population)
min(drug_data$crude_rate_added)
max(drug_data$crude_rate_added)
min(drug_data$percent_of_total_deaths)
max(drug_data$percent_of_total_deaths)
```

```{r}
drug_data <- drug_data %>%
  mutate(scaled_crude_rate_added = scale(crude_rate_added),
         scaled_percent_of_total_deaths = scale(percent_of_total_deaths),
         scaled_population = scale(population)
  )
```

```{r}
glimpse(drug_data)
```

```{r}
head(drug_data)
```

```{r}
model_nbinom2 <- glmmTMB(deaths ~ gender + race + year + ten_year_age_groups + 
                           scaled_population + scaled_percent_of_total_deaths,
                         data = drug_data, 
                         family = nbinom2(link = "log"))
```

```{r}
summary(model_nbinom2)
```

And perfect, we fixed our NA coefficient issues by sacling those predictors.

And try nbinom1 in case it's a better bet.

```{r}
model_nbinom1 <- glmmTMB(deaths ~ gender + race + year + ten_year_age_groups +
                           scaled_population + scaled_percent_of_total_deaths,
                         data = drug_data, 
                         family = nbinom1(link = "log"))
```

```{r}
summary(model_nbinom1)
```

And NA for AIC/BIC values...

Hmm, what if we added "County" as a random effect to our model?  This would get around the issue of there being 82 counties which puts us over the parameter limit.

```{r}
drug_data_arrange <- drug_data %>% arrange(county)
```

```{r}
model_nbinom2_re <- glmmTMB(deaths ~ gender + race + year + ten_year_age_groups + 
                              scaled_population + scaled_percent_of_total_deaths + (1 | county),
                            data = drug_data_arrange, 
                            family = nbinom2(link = "log"))
```

```{r}
summary(model_nbinom2_re)
```

But, Prof. DeRuiter did mention it doesn't make sense to have any random effects in our scenario.

___

#### Model Selection

```{r}
AIC(model_nbinom1, model_nbinom2, model_nbinom2_re)
```

```{r}
BIC(model_nbinom1, model_nbinom2, model_nbinom2_re)
```

And we're going to go with the nbinom2 model without a random effect.

```{r}
glimpse(drug_data)
```

Based on dredge, we should keep "gender", "race", "scaled_percent_of_total_deaths", "scaled_population", and "ten_year_age_group".

"Year" should be dropped from the model.

```{r}
dredge(model_nbinom2)
```

Wow, that took a minute or two to run.

```{r}
drop1(model_nbinom2, k = 2)
```

Based on AIC, we should drop "Year" as AIC values decrease when removing this predictor.

```{r}
drop1(model_nbinom2, k = log(nrow(drug_data)))
```

Based on BIC, we should drop "single_year_ages_factor"Year" and "ten_year_age_groups" as BIC values decrease when removing this predictor.

```{r}
Anova(model_nbinom2)
```

Based on Anova, "Year" should also be removed as a predictor in the model.

___

# Model Condition Checking.

```{r}
overdisp_fun(model_nbinom2)
```

Over-dispersion looks good as it's below 2-3 units.

```{r}
gf_acf(~model_nbinom2)
```

```{r}
gf_acf(~model_nbinom2) %>% gf_lims(y = c(-0.2, 0.2), x = c(0, 30))
```

And we have some issues with independence of residuals at two of the lags.  This doesn't look terrible though.


```{r}
model_sim <- simulateResiduals(model_nbinom2, n = 1000)
```

```{r}
gf_point(model_sim$scaledResiduals ~ fitted(model_nbinom2)) %>%
  gf_labs(x = "Predicted # of Deaths",
          y = 'Scaled Residuals')
```

And this definitely isn't a uniform distribution.  I'm going to guess we have issues with linearity and possibly mean-variance.

Could also check log(response variable) vs. predictors and scaled residuals vs. predictors but at this point it's clear this model fails.

___

# Prediction plots.

```{r}
ggpredict(model_nbinom2)
```

```{r}
# t(get_fixed(logit_model_no_offset)) %>% pander()
```

```{r}
# get_fixed(model_nbinom2)
```

The other two functions to obtain fixed values for predictors fails. So, going with ggpredict().

```{r}
glimpse(drug_data)
```

Prediction plot for "gender":

And the long-winded manual way...

```{r}
# note: "chr" variables have unique() values, "fct" variables have level()s
gender_prediction_data <- expand.grid(
  # char_variable = pull(police_stop, char_variable) %>% unique(),
  # quantitative_variable = seq(from = 0, to = 10000, by = 100),
  # categorical_variable = pull(police_stop, categorical_variable) %>% factor() %>% levels(),
  gender = pull(drug_data, gender) %>% factor() %>% levels(),
  race = "Black or African American",
  year = "1999",
  ten_year_age_groups = "15-24 years",
  scaled_population = -0.00,
  scaled_percent_of_total_deaths = 0.00)

# compute predictions with SEs
gender_predictions <- predict(model_nbinom2, 
                              newdata = gender_prediction_data,
                              type = 'response', se.fit = TRUE)

glimpse(gender_predictions)
```

```{r}
# add predicted values and CIs to the hypothetical dataset
gender_prediction_data <- gender_prediction_data %>%
  mutate(predictions = gender_predictions$fit,
         CI_low = gender_predictions$fit - 1.96 * gender_predictions$se.fit,
         CI_hi = gender_predictions$fit + 1.96 * gender_predictions$se.fit)
```

```{r}
gf_point(predictions ~ gender, data = gender_prediction_data,
         alpha = 1.0, color = "black",
         ylab="Predicted (fitted) Values of Deaths",
         xlab="Gender",
         title="CDC Wonder Data - Prediction Plot",
         subtitle = "",
         caption = ""
) %>%
  # gf_ribbon(CI_low + CI_hi ~ single_year_ages_factor) %>%
  gf_errorbar(CI_low + CI_hi ~ gender) %>% gf_lims(y = c(15, 25))
```

Prediction plot for "race":

```{r}
ggpredict(model_nbinom2, 
          terms = 'race',
          type = 'fixed') %>% plot()
```

Prediction plot for "year":

```{r}
ggpredict(model_nbinom2, 
          terms = 'year',
          type = 'fixed') %>% plot()
```

Prediction plot for "ten_year_age_groups":

```{r}
ggpredict(model_nbinom2, 
          terms = 'ten_year_age_groups',
          type = 'fixed') %>% plot()
```

Prediction plot for "population":

```{r}
ggpredict(model_nbinom2, 
          terms = 'scaled_population',
          type = 'fixed') %>% plot()
```
Prediction plot for "scaled_percent_of_total_deaths":

```{r}
ggpredict(model_nbinom2, 
          terms = 'scaled_percent_of_total_deaths',
          type = 'fixed') %>% plot()
```

___

# Model fitting (3rd research question)

Convert "cause_of_death" to a binary response variable in preparation for logistic regression modeling.

```{r}
unique(data$cause_of_death)
```

Convert to binary response and store in new variable.  Then, reassign as "police_stop".
```{r}
all_data <- data %>% mutate(drug_death = case_when(
  cause_of_death == "drug" ~ "yes",
  TRUE ~ "no"))
```

Convert response to a factor.

```{r}
all_data <- all_data %>% mutate(drug_death = as.factor(drug_death))
```

```{r}
all_data <- all_data %>% mutate(drug_death = fct_relevel(drug_death, c("no", "yes")))
```

```{r}
levels(all_data$drug_death)
```
```{r}
glimpse(all_data)
```

And seems like we're good to go.

Adjustment of Rule of Thumb for # of parameters for Logistic (binary) regression:
```{r}
p <- min(sum(data$drug_death == 'yes'), 
         sum(data$drug_death == 'no'))
p
p / 15
```

And similar to our negative binomial model, we can have at most 45 parameters.

Compute # of parameters used in model (1 for each quantitative, levels - 1 for each categorical, 1 each for intercept/sigma).

```{r}
length(levels(drug_data$gender)) - 1 + length(levels(drug_data$race)) - 1 + 
  length(levels(drug_data$year)) - 1 + length(levels(drug_data$ten_year_age_groups)) - 1 + 
  1 + 1 + 1 + 1  + 3 # 1 each for deaths, population, percent_of_total_deaths, residuals; 3 for interaction.
```

Double-check we have no rows with NA's.

```{r}
nrow(all_data)
```

```{r}
all_data_dropNAs <- all_data %>% na.omit()
nrow(all_data_dropNAs)
```

And all set.

```{r}
sum(all_data$drug_death == "yes")
sum(all_data$drug_death == "no")
```

We have far more non-drug death related data.

```{r}
model_test <- glmmTMB(drug_death ~ gender + percent_of_total_deaths + deaths + crude_rate_added + 
                        year + population,
                      data = all_data, family = binomial(link = 'logit'))
summary(model_test)
```

Adding "race" or "ten_year_age_groups" causes the coefficients to go straight to all NAs.  This limits us rather drastically...

Let's try glm() instead to see if there's any difference.

```{r}
model_test2 <- glm(drug_death ~ gender + race + year + ten_year_age_groups + 
                     population + deaths + percent_of_total_deaths + gender*race,
                   data = all_data, family = binomial(link = 'logit'))
summary(model_test2)
```

Ok, glm() doesn't have the issue with NAs that glmTMB() does with NAs.  I have no idea why but as we're not using random effects I might as well go with glm.

```{r}
model_logit <- glm(drug_death ~ gender + race + year + ten_year_age_groups + 
                     population + deaths + percent_of_total_deaths + gender*race,
                   data = all_data, family = binomial(link = 'logit'), na.action = "na.fail")
```

```{r}
summary(model_logit)
```

Hmm, let's try re-scaling our data again.

```{r}
all_data <- all_data %>%
  mutate(scaled_crude_rate_added = scale(crude_rate_added),
         scaled_percent_of_total_deaths = scale(percent_of_total_deaths),
         scaled_population = scale(population),
         scaled_deaths = scale(deaths)
  )
```

```{r}
glimpse(all_data)
```


```{r}
model_logit_scaled <- glm(drug_death ~ gender + race + year + ten_year_age_groups + 
                     scaled_population + scaled_deaths + scaled_percent_of_total_deaths + gender*race,
                   data = all_data, family = binomial(link = 'logit'))
```

```{r}
summary(model_logit_scaled)
```

And looks exactly the same scaled or un-scaled.
___

Confidence intervals for the log(odds-ratio).
```{r}
model_CIs <- confint(model_logit)
```

```{r}
model_CIs
```

So, all the predictors/parameters whose estimate crosses through 0 could have no relationship with the response variable (takes a while to run this function).

___

Confidence intervals for the odds-ratio.
```{r}
exp(model_CIs)
```

So, all the predictors/parameters whose estimate crosses through 1 could have no relationship with the response variable.

___

#### Model Selection

```{r}
AIC(model_test, model_logit, model_logit_scaled)
```

```{r}
BIC(model_test, model_logit, model_logit_scaled)
```

And we're going to go with the un-scaled logit model utilizing glm().

Based on dredge, we should keep "death", "percent_of_total_deaths", "population", "race", "ten_year_age_group", "year", and maybe "gender".

"gender*race" can be dropped from the model.

```{r}
dredge(model_logit)
```

Takes a little bit of time to run dredge().

```{r}
drop1(model_logit, k = 2)
```

Based on AIC, we should drop "gender*race" as AIC values decrease when removing this predictor.

```{r}
drop1(model_logit, k = log(nrow(all_data)))
```

Based on BIC, we should drop "year" and "gender*race" as BIC values decrease when removing this predictor.

```{r}
Anova(model_logit)
```

Based on Anova, "gender" and "gender*race" can be removed as predictors in the model.

___

# Model Condition Checking.

According to Week 7 slides on Binary Regression...

* Response variable is logical (data can be proportion but not binary!)
* Linearity: logit(p) should have a linear relationship with each predictor variable. (A bit hard to check - can bin predictor and then plot scatter plot)
* Independence of Residuals: Same as usual.
* Mean-variance relationship: scaled residuals have uniform distribution and no trends.
* NO distributional assumptions about residuals.

___

Our response variable "drug_death" is logical (since I made it so).  So, this check passes with flying colors.

___

Auto-Correlation Plots:

```{r}
acf(resid(model_logit))
```

Ok, that fails miserably right from the start.

Attempt to fix...

```{r}
require(lme4)
```

```{r}
model_logit_fix_attempt <- glmer(drug_death ~ gender + race + year + ten_year_age_groups + 
                     population + deaths + percent_of_total_deaths + gender*race + (1 | county),
                   data = all_data, family = binomial(link = 'logit'), na.action = "na.fail", nAGQ = 0)
```

And that took forever to finish...

```{r}
msummary(model_logit_fix_attempt)
```

Well, at least it's not giving NA's like with glmmTMB.

```{r}
acf(resid(model_logit_fix_attempt))
```

And that didn't do anything.  Time to ask Prof. DeRuiter for help.

Maybe a logistic regression model with a binary response isn't suitable in this scenario?

Continuing on for kicks and giggles though.

___

DHARMa package scaled residuals versus fitted (predicted) plot:

```{r}
model_logit_sim <- simulateResiduals(model_logit, n = 1000)
```

```{r}
gf_point(model_logit_sim$scaledResiduals ~ fitted(model_logit)) %>%
  gf_labs(x = "Predicted Probability for 'yes' is a drug-related",
          y = 'Scaled Residuals')
```

Mean-Variance relationship does seem okay as this looks like a uniform distribution.  Linearity should be fine too from a glance though the area in the lower middle is a little sparse compared to the rest.

___

#### Plot each quantitative predictor binned against the response.

Code in the section below just checks that we are binning correctly and does some other prep in preparation for the linearity check plots.

```{r}
all_data_binned <- all_data %>%
  mutate(binned_deaths = cut_number(deaths, 10)) 
```

```{r}
all_data_binned_levels <- unique(all_data_binned$binned_deaths)
all_data_binned_levels
all_data_binned_observations = c()
```

```{r}
for (element in all_data_binned_levels) {
  print(element)
  value <- nrow(filter(all_data_binned, binned_deaths == element))
  print(value)
  all_data_binned_observations <- append(all_data_binned_observations, value)
}
print(all_data_binned_observations)
all_data_binned_observations <- c()
```

Bins look pretty good in terms of the number of available observations in each bin.

```{r}
median(all_data$deaths)
class(median(all_data$deaths))
```

```{r}
prop(all_data$drug_death == "yes")
class(prop(all_data$drug_death == "yes"))
```

```{r}
glimpse(all_data_binned)
```

___

```{r}
all_data_grouped <- all_data %>%
  mutate(binned_deaths = cut_number(deaths, 10)) %>%
  group_by(binned_deaths) %>%
  summarize(proportion_drug_deaths = prop(drug_death == "yes"),
            median_deaths = median(deaths))
```

```{r}
head(all_data_grouped, 5)
```

```{r}
gf_point(proportion_drug_deaths ~ median_deaths, data = all_data_grouped,
         alpha = 1.0, color = "black",
         ylab="Proportion where drug_death == 'yes'",
         xlab="Median of 'deaths' in each Bin",
         title="CDC Wonder Underlying Cause of Death Dataset Queries",
         subtitle = "",
         caption = ""
) %>% gf_lm()
```

```{r}
gf_point(logit(proportion_drug_deaths) ~ median_deaths, data = all_data_grouped,
         alpha = 1.0, color = "black",
          ylab="Proportion where drug_death == 'yes'",
         xlab="Median of 'deaths' in each Bin",
         title="CDC Wonder Underlying Cause of Death Dataset Queries",
         subtitle = "",
         caption = ""
) %>% gf_lm()
```

Hmm, not quite a linear relationship.

___

```{r}
all_data_binned <- all_data %>%
  mutate(binned_population = cut_number(population, 10)) 
```

```{r}
all_data_binned_levels <- unique(all_data_binned$binned_population)
all_data_binned_levels
all_data_binned_observations = c()
```

```{r}
for (element in all_data_binned_levels) {
  print(element)
  value <- nrow(filter(all_data_binned, binned_population == element))
  print(value)
  all_data_binned_observations <- append(all_data_binned_observations, value)
}
print(all_data_binned_observations)
all_data_binned_observations <- c()
```

Bins look pretty good in terms of the number of available observations in each bin.

___

```{r}
all_data_grouped <- all_data %>%
  mutate(binned_population = cut_number(population, 10)) %>%
  group_by(binned_population) %>%
  summarize(proportion_drug_deaths = prop(drug_death == "yes"),
            median_population = median(population))
```

```{r}
head(all_data_grouped, 5)
```

```{r}
gf_point(proportion_drug_deaths ~ median_population, data = all_data_grouped,
         alpha = 1.0, color = "black",
         ylab="Proportion where drug_death == 'yes'",
         xlab="Median of 'population' in each Bin",
         title="CDC Wonder Underlying Cause of Death Dataset Queries",
         subtitle = "",
         caption = ""
) %>% gf_lm()
```

```{r}
gf_point(logit(proportion_drug_deaths) ~ median_population, data = all_data_grouped,
         alpha = 1.0, color = "black",
         ylab="Logit(Proportion) where drug_death == 'yes'",
         xlab="Median of 'population' in each Bin",
         title="CDC Wonder Underlying Cause of Death Dataset Queries",
         subtitle = "",
         caption = ""
) %>% gf_lm()
```

Yea...

___

```{r}
all_data_binned <- all_data %>%
  mutate(binned_percent_of_total_deaths = cut_number(percent_of_total_deaths, 10)) 
```

```{r}
all_data_binned_levels <- unique(all_data_binned$binned_percent_of_total_deaths)
all_data_binned_levels
all_data_binned_observations = c()
```

```{r}
for (element in all_data_binned_levels) {
  print(element)
  value <- nrow(filter(all_data_binned, binned_percent_of_total_deaths == element))
  print(value)
  all_data_binned_observations <- append(all_data_binned_observations, value)
}
print(all_data_binned_observations)
all_data_binned_observations <- c()
```

Bins look pretty good in terms of the number of available observations in each bin.

___

```{r}
all_data_grouped <- all_data %>%
  mutate(binned_percent_of_total_deaths = cut_number(percent_of_total_deaths, 10)) %>%
  group_by(binned_percent_of_total_deaths) %>%
  summarize(proportion_drug_deaths = prop(drug_death == "yes"),
            median_percent_of_total_deaths = median(percent_of_total_deaths))
```

```{r}
head(all_data_grouped, 5)
```

```{r}
gf_point(proportion_drug_deaths ~ median_percent_of_total_deaths, data = all_data_grouped,
         alpha = 1.0, color = "black",
         ylab="Proportion where drug_death == 'yes'",
         xlab="Median of 'percent_of_total_deaths' in each Bin",
         title="CDC Wonder Underlying Cause of Death Dataset Queries",
         subtitle = "",
         caption = ""
) %>% gf_lm()
```

```{r}
gf_point(logit(proportion_drug_deaths) ~ median_percent_of_total_deaths, data = all_data_grouped,
         alpha = 1.0, color = "black",
         ylab="Logit(Proportion) where drug_death == 'yes'",
         xlab="Median of 'percent_of_total_deaths' in each Bin",
         title="CDC Wonder Underlying Cause of Death Dataset Queries",
         subtitle = "",
         caption = ""
) %>% gf_lm()
```


Hmm, just noticed that you might be able to drive percent_of_total_deaths by deaths / population similarly to how crude_rate is deaths / population * 1000.

Will look into it at some point.

___

# Prediction plots.

Get the fixed values we will be using for the plots.

```{r}
ggpredict(model_logit)
```

```{r}
t(get_fixed(model_logit)) %>% pander()
```

```{r}
get_fixed(model_logit)
```

```{r}
glimpse(all_data)
```

And our model is obviously terrible.

Prediction plot for "gender":

```{r}
ggpredict(model_logit, 
          terms = 'gender',
          type = 'fixed') %>% plot()
```

Prediction plot for "race":

```{r}
ggpredict(model_logit, 
          terms = 'race',
          type = 'fixed') %>% plot()
```

Prediction plot for "year":

```{r}
ggpredict(model_logit, 
          terms = 'year',
          type = 'fixed') %>% plot()
```

Prediction plot for "ten_year_age_groups":

```{r}
ggpredict(model_logit, 
          terms = 'ten_year_age_groups',
          type = 'fixed') %>% plot()
```

Prediction plot for "deaths":

```{r}
ggpredict(model_logit, 
          terms = 'deaths',
          type = 'fixed') %>% plot()
```

Prediction plot for "population":

```{r}
ggpredict(model_logit, 
          terms = 'population',
          type = 'fixed') %>% plot()
```

Prediction plot for "scaled_percent_of_total_deaths":

```{r}
ggpredict(model_logit, 
          terms = 'percent_of_total_deaths',
          type = 'fixed') %>% plot()
```

Prediction plot for "gender*race":

```{r}
ggpredict(model_logit, 
          terms = c('gender', 'race'),
          type = 'fixed') %>% plot()
```

___
___
___


